{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://ibm.box.com/shared/static/9gegpsmnsoo25ikkbl4qzlvlyjbgxs5x.png\" width = 400> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals I - Introduction to Spark </h1>\n",
    "<h2 align = \"center\"> Python - Working with RDD operations </h2>\n",
    "<br align = \"left\">\n",
    "\n",
    "**Related free online courses:**  \n",
    "\n",
    "Related courses can be found in the following learning paths:\n",
    "\n",
    "- [Spark Fundamentals path](http://cocl.us/Spark_Fundamentals_Path)\n",
    "- [Big Data Fundamentals path](http://cocl.us/Big_Data_Fundamentals_Path)\n",
    "\n",
    "<img src = \"http://spark.apache.org/images/spark-logo.png\", height = 100, align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Analyzing a log file\n",
    "\n",
    "First, let's download the data that we will working with in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded!\n"
     ]
    }
   ],
   "source": [
    "# download the data from the IBM server\n",
    "# this may take ~30 seconds depending on your interent speed\n",
    "!wget --quiet https://ibm.box.com/shared/static/j8skrriqeqw66f51iyz911zyqai64j2g.zip\n",
    "print(\"Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted!\n"
     ]
    }
   ],
   "source": [
    "# unzip the folder's content into \"resources\" directory\n",
    "# this may take ~30 seconds depending on your internet speed\n",
    "!unzip -q -o -d /resources/jupyterlab/labs/BD0211EN/ j8skrriqeqw66f51iyz911zyqai64j2g.zip\n",
    "print(\"Data Extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "followers.txt\n",
      "notebook.log\n",
      "nyctaxi.csv\n",
      "nyctaxi100.csv\n",
      "nyctaxisub.csv\n",
      "nycweather.csv\n",
      "pom.xml\n",
      "taxistreams.py\n",
      "users.txt\n"
     ]
    }
   ],
   "source": [
    "# list the extracted files\n",
    "!ls -1 /resources/jupyterlab/labs/BD0211EN/LabData/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an RDD by loading the log file that we analyze in the Scala version of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/notebook.log\")\n",
    "type(logFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <span style=\"color: red\">YOUR TURN:</span> \n",
    "\n",
    "#### In the cell below, filter out the lines that contains INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15/10/14 14:29:21 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/14 14:29:23 INFO Slf4jLogger: Slf4jLogger started']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE BELOW\n",
    "info = logFile.filter(lambda line: 'INFO' in line)\n",
    "type(info)\n",
    "info.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info = logFile.filter(lambda line: \"INFO\" in line)\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Count the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE BELOW\n",
    "info.collect().__len__()\n",
    "# info.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.count()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Count the lines with \"spark\" in it by combining transformation and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE BELOW\n",
    "logFile.filter(lambda line: 'spark' in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.filter(lambda line: \"spark\" in line).count()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Fetch those lines as an array of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/14 14:29:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53333]',\n",
       " \"15/10/14 14:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 53333.\",\n",
       " '15/10/14 14:29:23 INFO DiskBlockManager: Created local directory at /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a',\n",
       " '15/10/14 14:29:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/httpd-ed3f4ab0-7218-48bc-9d8a-3981b1cfe574',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35726.\",\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/15 15:33:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:47412]',\n",
       " \"15/10/15 15:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 47412.\",\n",
       " '15/10/15 15:33:42 INFO DiskBlockManager: Created local directory at /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE BELOW\n",
    "# logFile.filter(lambda line: 'spark' in line).collect()\n",
    "logFile.filter(lambda line: 'spark' in line).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.filter(lambda line: \"spark\" in line).collect()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "View the graph of an RDD using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[8] at collect at <ipython-input-12-20a5af6a67d2>:2 []\\n |  /resources/jupyterlab/labs/BD0211EN/LabData/notebook.log MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  /resources/jupyterlab/labs/BD0211EN/LabData/notebook.log HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "print(info.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Joining RDDs\n",
    "\n",
    "Next, you are going to create RDDs for the same README and the POM files that we used in the Scala version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "readmeFile = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/README.md\")\n",
    "pomFile = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/pom.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How many Spark keywords are in each file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Lines with spark keyword\n",
    "# print(readmeFile.filter(lambda line: \"Spark\" in line).count())\n",
    "# print(pomFile.filter(lambda line: \"Spark\" in line).count())\n",
    "# readmeFile.filter(lambda line: \"Spark\" in line).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the occurence of Spark word\n",
    "spark_line = readmeFile.filter(lambda line: \"Spark\" in line)\n",
    "spark_word = spark_line.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "spark_word.sortByKey().lookup('Spark')\n",
    "\n",
    "pom_line = pomFile.filter(lambda line: \"Spark\" in line)\n",
    "pom_word = pom_line.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "pom_word.sortByKey().lookup('Spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now do a WordCount on each RDD so that the results are (K,V) pairs of (word,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "myReadmeCount = readmeFile.flatMap(lambda line: line.split()). \\\n",
    "            map(lambda word: (word, 1)). \\\n",
    "            reduceByKey(lambda a, b: a + b)\n",
    "# myReadmeCount.collect()\n",
    "\n",
    "myPomCount = readmeFile.flatMap(lambda line: line.split()). \\\n",
    "            map(lambda word: (word, 1)). \\\n",
    "            reduceByKey(lambda a, b: a + b)\n",
    "# myReadmeCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see the array for either of them, just call the collect function on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme Count\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Readme Count\\n\")\n",
    "# print(readmeCount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pom Count\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Pom Count\\n\")\n",
    "# print(pomCount.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The join function combines the two datasets (K,V) and (K,W) together and get (K, (V,W)). Let's join these two counts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# joined = readmeCount.join(pomCount)\n",
    "joined = myReadmeCount.join(myPomCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print the value to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', (1, 1)),\n",
       " ('Apache', (1, 1)),\n",
       " ('Spark', (14, 14)),\n",
       " ('is', (6, 6)),\n",
       " ('It', (2, 2)),\n",
       " ('provides', (1, 1)),\n",
       " ('high-level', (1, 1)),\n",
       " ('APIs', (1, 1)),\n",
       " ('in', (5, 5)),\n",
       " ('Scala,', (1, 1))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joined.collect()\n",
    "joined.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's combine the values together to get the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 1),\n",
       " (14, 14),\n",
       " (6, 6),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (5, 5),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-D join table: add the 1st and 2nd value of tuple\n",
    "# k[0] -> key | # k[1] -> value tuple | # k[1][0] and k[1][1] are two values of the value pair\n",
    "# so to add valuees of the value pair -> k[1][0] + k[1][1]\n",
    "\n",
    "joinedSum = joined.map(lambda k: (k[0], (k[1][0]+k[1][1])))\n",
    "# joined.map(lambda k: k[1]).take(10)\n",
    "joined.map(lambda k: k[1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To check if it is correct, print the first five elements from the joined and the joinedSum RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Individial\n",
      "\n",
      "[('#', (1, 1)), ('Apache', (1, 1)), ('Spark', (14, 14)), ('is', (6, 6)), ('It', (2, 2))]\n",
      "\n",
      "\n",
      "Joined Sum\n",
      "\n",
      "[('#', 2), ('Apache', 2), ('Spark', 28), ('is', 12), ('It', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Joined Individial\\n\")\n",
    "print(joined.take(5))\n",
    "\n",
    "print(\"\\n\\nJoined Sum\\n\")\n",
    "print(joinedSum.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Shared variables\n",
    "\n",
    "Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.\n",
    "\n",
    "### Broadcast variables\n",
    "\n",
    "Broadcast variables are useful for when you have a large dataset that you want to use across all the worker nodes. A read-only variable is cached on each machine rather than shipping a copy of it with tasks. Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage.\n",
    "\n",
    "\n",
    "Read more here: [http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables)\n",
    "\n",
    "Create a broadcast variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get the value, type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Accumulators\n",
    "\n",
    "Accumulators are variables that can only be added through an associative operation. It is used to implement counters and sum efficiently in parallel. Spark natively supports numeric type accumulators and standard mutable collections. Programmers can extend these for new types. Only the driver can read the values of the accumulators. The workers can only invoke it to increment the value.\n",
    "\n",
    "Create the accumulator variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next parallelize an array of four integers and run it through a loop to add each integer value to the accumulator variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, iterate through each element of the rdd and apply the function f on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get the current value of the accumulator variable, type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should get a value of 10.\n",
    "\n",
    "This command can only be invoked on the driver side. The worker nodes can only increment the accumulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Key-value pairs\n",
    "\n",
    "You have already seen a bit about key-value pairs in the Joining RDD section.\n",
    "\n",
    "Create a key-value pair of two characters. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pair = ('a', 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To access the value of the first index use [0] and [1] method for the 2nd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(pair[0])\n",
    "print(pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part is copied from Scala lab\n",
    "## Sample Application\n",
    "In this section, you will be using a subset of a data for taxi trips that will determine the top 10 medallion numbers based on the number of trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/my_nytaxi_short.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the five rows of content, invoke the take function. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29b3f4a30dea6688d4c289c9672cb996,1-ddfdec8050c7ef4dc694eeeda6c4625e,1/11/2013 22:03,4.07E+01,-7.40E+01,A93D1F7F8998FFB75EEF477EB6077516,68BC16A99E915E44ADA7E639B4DD5F59,2,1/11/2013 21:48,4.07E+01,-7.40E+01,1,,4.08E+00,900,VTS',\n",
       " '2a80cfaa425dcec0861e02ae44354500,1-b72234b58a7b0018a1ec5d2ea0797e32,1/11/2013 4:28,4.08E+01,-7.39E+01,64CE1B03FDE343BB8DFB512123A525A4,60150AA39B2F654ED6F0C3AF8174A48A,1,1/11/2013 4:07,4.07E+01,-7.40E+01,1,,8.53E+00,1260,VTS',\n",
       " '29b3f4a30dea6688d4c289c96758d87e,1-387ec30eac5abda89d2abefdf947b2c1,1/11/2013 22:02,4.07E+01,-7.40E+01,2D73B0C44F1699C67AB8AE322433BDB7,6F907BC9A85B7034C8418A24A0A75489,5,1/11/2013 21:46,4.08E+01,-7.40E+01,1,,3.01E+00,960,VTS',\n",
       " '2a80cfaa425dcec0861e02ae446226e4,1-aa8b16d6ae44ad906a46cc6581ffea50,1/11/2013 10:03,4.08E+01,-7.40E+01,E90018250F0A009433F03BD1E4A4CE53,1AFFD48CC07161DA651625B562FE4D06,5,1/11/2013 9:44,4.07E+01,-7.40E+01,1,,3.64E+00,1140,VTS',\n",
       " '29b3f4a30dea6688d4c289c9675a019c,1-dc8295eae03262a84370b8a6450eb38e,1/11/2013 22:02,4.07E+01,-7.40E+01,A4905117273C479965F469A85DE2363D,8BF138EA0CF6FF83587993BECA6D6D59,1,1/11/2013 21:48,4.08E+01,-7.40E+01,1,,3.69E+00,840,VTS']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that the first line is the headers. Normally, you would want to filter that out, but since it will not affect our results, we can leave it in.\n",
    "\n",
    "##### To parse out the values, including the medallion numbers, you need to first create a new RDD by splitting the lines of the RDD using the comma as the delimiter. Type in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiParse = taxi.map(lambda line: line.split(','))\n",
    "# taxiParse.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement: Find no of trips a particular taxi took | See which taxi took the most trips\n",
    "\n",
    "##### Now create the key-value pairs where the key is the medallion number and the value is 1. We use this model to later sum up all the keys to find out the number of trips a particular taxi took and in particular, will be able to see which taxi took the most trips. Map each of the medallions to the value of one. Type in:\n",
    "Note: medallion is the 7th field ie at index 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below Creating \n",
    "##### 1) Map: tuple of (medallion no, (1, dist, duration))\n",
    "##### 2) Reduce: tuple of (occurence, sum of dist travelled, sum of duration of trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68BC16A99E915E44ADA7E639B4DD5F59', (1, 2, 900)),\n",
       " ('60150AA39B2F654ED6F0C3AF8174A48A', (1, 1, 1260)),\n",
       " ('6F907BC9A85B7034C8418A24A0A75489', (1, 5, 960)),\n",
       " ('1AFFD48CC07161DA651625B562FE4D06', (1, 5, 1140)),\n",
       " ('8BF138EA0CF6FF83587993BECA6D6D59', (1, 1, 840)),\n",
       " ('23A8ED0AAA1936A28C652B80903B42FB', (1, 1, 1260)),\n",
       " ('42BC02EC8FC9719B5B8075C3029B9EE9', (1, 1, 780)),\n",
       " ('EB49CE1B3661EF6100CF9EA1B860932E', (1, 1, 900)),\n",
       " ('DDE6F0B0832FA5CCE8491924E360FB45', (1, 1, 900)),\n",
       " ('68BC16A99E915E44ADA7E639B4DD5F59', (1, 5, 900))]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taxiMedKey = taxiParse.map(lambda x: (x[6], 1),)\n",
    "taxiMedKey = taxiParse.map(lambda x: (x[6], (1, int(x[7]), int(x[14]))))\n",
    "taxiMedKey.take(10)\n",
    "# taxiMedRed = taxiMedKey.reduceByKey(lambda a , b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "# taxiMedRed.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68BC16A99E915E44ADA7E639B4DD5F59', '25'),\n",
       " ('6F907BC9A85B7034C8418A24A0A75489', ('5', '960')),\n",
       " ('1AFFD48CC07161DA651625B562FE4D06', ('5', '1140')),\n",
       " ('8BF138EA0CF6FF83587993BECA6D6D59', ('1', '840')),\n",
       " ('23A8ED0AAA1936A28C652B80903B42FB', ('1', '1260'))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vals(6) corresponds to the column where the medallion key is located\n",
    "\n",
    "##### Next use the reduceByKey function to count the number of occurrence for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"DDE6F0B0832FA5CCE8491924E360FB45\"', 235),\n",
       " ('\"41DFF791640B9E2452F8FC0120C4A9F7\"', 229),\n",
       " ('\"BAE2E3D1E60161EB06CB10ACEE43F0E7\"', 270),\n",
       " ('\"28B0F9D039C707889FAB90A73377CDE4\"', 90),\n",
       " ('\"6D54E8A7CAC67485032E338D2322180B\"', 56)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiMedCounts = taxiMedKey.reduceByKey(lambda a, b: a + b)\n",
    "taxiMedCounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, the values are swapped so they can be ordered in descending order and the results are presented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi Medallion \"FE4C521F3C1AC6F2598DEF00DDD43029\" has 415 trips.\n",
      "Taxi Medallion \"F5BB809E7858A669C9A1E8A12A3CCF81\" has 411 trips.\n",
      "Taxi Medallion \"8CE240F0796D072D5DCFE06A364FB5A0\" has 406 trips.\n",
      "Taxi Medallion \"0310297769C8B049C0EA8E87C697F755\" has 402 trips.\n",
      "Taxi Medallion \"B6585890F68EE02702F32DECDEABC2A8\" has 399 trips.\n",
      "Taxi Medallion \"33955A2FCAF62C6E91A11AE97D96C99A\" has 395 trips.\n",
      "Taxi Medallion \"4F7C132D3130970CFA892CC858F5ECB5\" has 391 trips.\n",
      "Taxi Medallion \"78833E177D45E4BC520222FFBBAC5B77\" has 383 trips.\n",
      "Taxi Medallion \"E097412FE23295A691BEEE56F28FB9E2\" has 380 trips.\n",
      "Taxi Medallion \"C14289566BAAD9AEDD0751E5E9C73FBD\" has 377 trips.\n"
     ]
    }
   ],
   "source": [
    "taxiMedTop10 = taxiMedCounts.map(lambda x: (x[1], x[0])).top(10)\n",
    "taxiMedTop10\n",
    "for item in taxiMedTop10:\n",
    "    print(\"Taxi Medallion {1} has {0} trips.\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### While each step above was processed one line at a time, you can just as well process everything on one line:\n",
    "##### Find count of trips in two ways 1) reduceByKey on rddtuple(k,1) and 2) combineByKey on rddtuple(k,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "# op1:\n",
    "taxiMedCountsOneLine1 = taxi.map(lambda line: line.split(',')).map(lambda x: (x[6], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# op2:\n",
    "taxiMedCountsOneLine2 = taxi.map(lambda line: line.split(',')).map(lambda x: (x[6], 1)).countByKey()\n",
    "# taxiMedCountsOneLine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the same line as above to print the taxiMedCountsOneLine RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(415, '\"FE4C521F3C1AC6F2598DEF00DDD43029\"'),\n",
       " (411, '\"F5BB809E7858A669C9A1E8A12A3CCF81\"'),\n",
       " (406, '\"8CE240F0796D072D5DCFE06A364FB5A0\"'),\n",
       " (402, '\"0310297769C8B049C0EA8E87C697F755\"'),\n",
       " (399, '\"B6585890F68EE02702F32DECDEABC2A8\"')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: When using combineByKey you can no longer swap as combineByKey is a action which returns collection:defaultdict\n",
    "taxiMedCountsOneLine1.map(lambda x: (x[1], x[0])).top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's cache the taxiMedCountsOneLine to see the difference caching makes. Run it with the logs set to INFO and you can see the output of the time it takes to execute each line. First, let's cache the RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taxiMedCountsOneLine.cache()\n",
    "type(taxiMedCountsOneLine.cache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, you have to invoke an action for it to actually cache the RDD. Note the time it takes here (either empirically using the INFO log or just notice the time it takes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiMedCountsOneLine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run it again to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13464"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiMedCountsOneLine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Header from csv file --> While Loading into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "taxi = spark.read.option(\"header\",\"true\").csv(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "type(taxi)\n",
    "type(taxi)\n",
    "# for item in taxi.take(10):\n",
    "#     print(item[6])\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Header from csv file --> WHile creating a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2677600"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "# taxi = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/my_nytaxi_short_wheader.csv\")\n",
    "# taxi.count()\n",
    "headertag = taxi.first()\n",
    "header = sc.parallelize([headertag])\n",
    "data = taxi.subtract(header)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [4, 5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # one_through_9 = range(1,10)\n",
    "one_through_9 = [1,2,3,4,5,6,7,8,9]\n",
    "parallel = sc.parallelize(one_through_9, 3)\n",
    "# parallel = sc.parallelize(one_through_9)\n",
    "# def f(iterator): yield sum(iterator)\n",
    "# def h(index, iterator): \n",
    "#     yield index, list(iterator)\n",
    "def h(index, iterator):\n",
    "    if index == 0:\n",
    "        yield index, list(iterator)[1:]\n",
    "    else:\n",
    "        yield index, list(iterator)\n",
    "    \n",
    "# parallel.mapPartitions(f).collect()             # -> map partition (iterator)\n",
    "parallel.mapPartitionsWithIndex(h).collect()      # -> map partition with index posn of partition. (index, iterator)         \n",
    "# sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The bigger the dataset, the more noticeable the difference will be. In a sample file such as ours, the difference may be negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Summary\n",
    "Having completed this exercise, you should now be able to describe Spark’s primary data abstraction, work with Resilient Distributed Dataset (RDD) operations, and utilize shared variables and key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook is part of the free course on **Cognitive Class** called *Spark Fundamentals I*. If you accessed this notebook outside the course, you can take this free self-paced course, online by going to: http://cocl.us/Spark_Fundamentals_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### About the Authors:  \n",
    "Hi! It's [Alex Aklson](https://www.linkedin.com/in/aklson/), one of the authors of this notebook. I hope you found this lab educational! There is much more to learn about Spark but you are well on your way. Feel free to connect with me if you have any questions.\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

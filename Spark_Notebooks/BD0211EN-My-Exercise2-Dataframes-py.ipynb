{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile_df = spark.read.text(r\"LabData/README.md\")\n",
    "textFile_df.head(2)\n",
    "textFile_df.count()\n",
    "textFile_df.first()\n",
    "type(textFile_df)\n",
    "\n",
    "# RDD ---\n",
    "# textFile2 = sc.textFile(r\"LabData/README.md\")\n",
    "# type(textFile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scala Code. creates Dataset\n",
    "# val textFile = spark.read.textFile(r\"LabData/README.md\")\n",
    "# textFile.head(5)\n",
    "# textFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Filter -> Filter lines which contains 'Spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark = textFile_df.filter(textFile_df.value.contains('Spark'))\n",
    "# linesWithSpark = textFile.filter(lambda line: 'Spark' in textFile) --> This wont work\n",
    "# linesWithSpark2 = textFile2.filter(lambda line: 'Spark' in line)\n",
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scala\n",
    "# val linesWithSpark = textFile.filter(line => line.contains(\"Spark\"))\n",
    "# linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Map --> use select in Dataframes. No Map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCount\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Word list per line\n",
    "wordList_df = textFile_df.select(split(textFile_df.value, \" \").name('wordList'))\n",
    "wordList_df.collect()\n",
    "wordList_df.show()\n",
    "\n",
    "\n",
    "# Word Count per line\n",
    "wordCount_Perline_df = textFile_df.select(size(split(textFile_df.value, \" \")).name('WordCountPLine'))\n",
    "wordCount_Perline_df.collect()\n",
    "max_WordCount_Perline_df = wordCount_Perline_df.agg(max(col(\"WordCountPLine\")))\n",
    "max_WordCount_Perline_df.collect()\n",
    "max_WordCount_Perline_df.show()\n",
    "\n",
    "\n",
    "# # Word Count Per word\n",
    "wordCount_PerWord_df = textFile_df.select(explode(split(textFile_df, \" \")).alias(\"WordCount\")).groupBy(\"WordCount\")\n",
    "wordCount_PerWord_df = textFile_df.select(explode(split(textFile_df.value, \"\\s+\")).name(\"word\")) \\\n",
    "                                .groupBy(\"word\").count().orderBy(desc(\"count\"))\n",
    "wordCount_PerWord_df.collect()\n",
    "wordCount_PerWord_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv file -> Different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using read.csv without/with option()\n",
    "df1 = spark.read.csv(r\"LabData/Err.csv\")\n",
    "df2 = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(r\"LabData/Err.csv\")\n",
    "\n",
    "# Using load() with option() and format()\n",
    "df3 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(r\"LabData/Err.csv\")\n",
    "\n",
    "print(df1.count())\n",
    "print(df2.count())\n",
    "print(df3.count())\n",
    "print(type(df1))\n",
    "print(type(df2))\n",
    "print(type(df3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulations --> Sample file 'Samp_MKT.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input csv file\n",
    "mkt_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(r\"LabData/Samp_MKT.csv\")\n",
    "mkt_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema of dataframe\n",
    "mkt_df.printSchema()\n",
    "# mkt_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column List and No of columns\n",
    "mkt_df.columns\n",
    "len(mkt_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Statistics of the dataframe - shows for numeric fields, otherwise null\n",
    "# mkt_df.describe().show()\n",
    "mkt_df.select('AMOUNT').describe().show()\n",
    "\n",
    "# # Non non numeric fields, it shows count, min, max. No mean or std\n",
    "mkt_df.select('EFFORT_KEY').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fields - one or multiple fields\n",
    "mkt_df.select('EFFORT_KEY').show(5)\n",
    "mkt_df.select('EFFORT_KEY', 'STATE').show(5)\n",
    "mkt_df.select(['EFFORT_KEY', 'STATE']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct --> for a column, for composite columns, for entire row\n",
    "# mkt_df.select('EFFORT-KEY').distinct().count()\n",
    "# Can also be used to dropduplicates. To drop dups you can also use dropduplicates() instead of distinct(). Both works\n",
    "\n",
    "mkt_df.select(['STATE','ZIP']).distinct().count()\n",
    "mkt_df.select(['STATE','ZIP']).dropDuplicates().count()\n",
    "mkt_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosswise Frequency | Choose atleast one categorical field if not both. Takes two arguments only. \n",
    "# Shows frequency of each value of 2nd argument column for 1st argument col in a matrix/Dataframe\n",
    "\n",
    "mkt_df.crosstab('STATE', 'AMOUNT').show()\n",
    "mkt_df.crosstab('FULFILL_STATUS', 'STATE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with null values | dropna() --> drop nulls | fillna() --> fill nulls with specified value(s)\n",
    "\n",
    "mkt_nonull_df = mkt_df.dropna()\n",
    "mkt_fillnull_df = mkt_df.fillna(-1)\n",
    "mkt_nonull_df.show()\n",
    "mkt_fillnull_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows based on values\n",
    "\n",
    "mkt_df.filter(mkt_df.AMOUNT > 500).select(['KEYLINE', 'AMOUNT']).show(5)\n",
    "mkt_df.filter((mkt_df.STATE != 'IL') & (mkt_df.AMOUNT > 300)).select('KEYLINE', 'STATE', 'AMOUNT').show(5)\n",
    "mkt_df.filter(mkt_df.AMOUNT > 500).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(EFFORT_KEY='TDPK', Sum_AMT=750), Row(EFFORT_KEY='TC5Q', Sum_AMT=11600)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "# GroupBY and aggregate functions\n",
    "\n",
    "# mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'mean'}).show(3)\n",
    "# mkt_df.groupBy(mkt_df.EFFORT_KEY).agg({'AMOUNT': 'std'}).show(3)\n",
    "# mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'max'}).show(3)\n",
    "# mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'min'}).show(3)\n",
    "# mkt_df.groupBy('EFFORT_KEY').count().collect()\n",
    "\n",
    "# mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'sum'}).show()\n",
    "# mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'sum'}).agg({'sum(AMOUNT)': 'max'}).show()\n",
    "\n",
    "\n",
    "df1 = mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'sum'})\n",
    "\n",
    "## --> Renaming the Agg of SUM(AMOUNT) to an alias 'Sum_AMT' | Two methods shown below\n",
    "\n",
    "df1 = mkt_df.groupBy('EFFORT_KEY').agg(sf.sum('AMOUNT').alias('Sum_AMT'))  # Method-1\n",
    "df1 = mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'sum'}).withColumnRenamed(\"SUM(AMOUNT)\", \"Sum_Amt\") # Method-2\n",
    "df1.take(2)\n",
    "\n",
    "\n",
    "# df2 = df1.sort('sum(AMOUNT)', ascending=False).select('EFFORT_KEY')\n",
    "# df2 = df1.sort('sum(AMOUNT)', ascending=False)\n",
    "# df2 = df1.sort('sum(AMOUNT)', ascending=False).select\n",
    "\n",
    "\n",
    "# list1 = df2.take(2)\n",
    "# print(list1)\n",
    "# for items in list1:\n",
    "#     mydict = items.asDict()\n",
    "#     print('Value for EFK:', mydict['EFFORT_KEY'], 'is ->', mydict['sum(AMOUNT)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderBy\n",
    "mkt_df.orderBy(mkt_df.CITY.desc()).select('KEYLINE', 'EFFORT_KEY', 'AMOUNT', 'CITY').show(5) \n",
    "\n",
    "# Note argument of orderBy does not properly work if the field contains '-'. Below doesn't work\n",
    "# You will get an error  ==> 'DataFrame' object has no attribute 'EFFORT'. So use '_' in col name instead of '-' as below\n",
    "# #-> mkt_df.orderBy(mkt_df.EFFORT-KEY.desc()).select('KEYLINE', 'EFFORT-KEY', 'AMOUNT', 'CITY').show(5) \n",
    "mkt_df.orderBy(mkt_df.EFFORT_KEY.desc()).select('KEYLINE', 'EFFORT_KEY', 'AMOUNT', 'CITY').show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBy with OrderBY\n",
    "\n",
    "mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'mean'}) \\\n",
    "        .orderBy(mkt_df.EFFORT_KEY.desc()).select('EFFORT_KEY', 'avg(AMOUNT)').show(5)\n",
    "\n",
    "mkt_df.groupBy('EFFORT_KEY').agg({'AMOUNT': 'mean'}) \\\n",
    "        .orderBy('avg(AMOUNT)').select('EFFORT_KEY', 'avg(AMOUNT)').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SQL Queries on Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_df.registerTempTable('MKT_TABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlContext.sql(\"SELECT KEYLINE, AMOUNT, STATE FROM MKT_TABLE WHERE NOT STATE = 'IL'\").show(10)\n",
    "sqlContext.sql(\"SELECT KEYLINE, AMOUNT, EFFORT_KEY, ZIP+4, STATE FROM MKT_TABLE WHERE NOT STATE = 'IL'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT EFFORT_KEY, AVG(AMOUNT) as Avg FROM MKT_TABLE GROUP BY EFFORT_KEY ORDER BY Avg Desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing on static files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# inputPath = \"/databricks-datasets/structured-streaming/events/file-0.json\"\n",
    "inputPath = r\"LabData/logs\"\n",
    "logSchema = StructType([StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType())])\n",
    "\n",
    "log_df = spark.read \\\n",
    "            .format(\"json\") \\\n",
    "            .schema(logSchema) \\\n",
    "            .load(inputPath)\n",
    "log_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countLog_df = log_df.groupBy(window(log_df.time, \"30 minutes\"), log_df.action).count()\n",
    "countLog_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamLog_df = spark.readStream \\\n",
    "                    .format(\"json\") \\\n",
    "                    .schema(logSchema) \\\n",
    "                    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "                    .load(inputPath)\n",
    "type(streamLog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingCountsDF = streamLog_df \\\n",
    "                .groupBy(streamLog_df.action, window(streamLog_df.time, \"1 hour\")) \\\n",
    "                .count()\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the stream into stream Dataframe\n",
    "logStream_df = spark.readStream \\\n",
    "                    .format(\"json\") \\\n",
    "                    .schema(logSchema) \\\n",
    "                    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "                    .load(inputPath)\n",
    "\n",
    "# group the Dataframe\n",
    "countStream_df = logStream_df.groupBy(window(logStream_df.time, \"30 minutes\"), logStream_df.action) \\\n",
    "                    .count()\n",
    "\n",
    "print('Am I streaming? -> ', countStream_df.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryy = countStream_df.writeStream \\\n",
    "                    .format(\"console\") \\\n",
    "                    .queryName(\"mylogs2\") \\\n",
    "                    .outputMode(\"complete\") \\\n",
    "                    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1.explain(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark.streams.active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mylogs2\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File file sinks Only Append mode is allowed. Append mode cannot work with Aggregates\n",
    "query_csv = logStream_df.writeStream \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"checkpointLocation\", r\"LabData/chk_pt\") \\\n",
    "                    .option(\"path\", r\"LabData/out\") \\\n",
    "                    .queryName(\"mylogs_csv\") \\\n",
    "                    .outputMode(\"append\") \\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_csv.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
